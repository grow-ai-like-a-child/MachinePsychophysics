<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Machine Psychophysics: Cognitive Control in Vision‚ÄìLanguage Models</title>

  <!-- Â≠ó‰Ωì‰∏éÊ†∑Âºè -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />
  <link rel="stylesheet" href="index.css" />

  <!-- FaviconÔºàÂèØËá™ÂÆö‰πâÔºâ -->
  <link rel="icon" href="favicon.svg" />
</head>
<body>

  <!-- Header -->
  <header>
    <h1>Machine Psychophysics</h1>
    <h2>Cognitive Control in Vision‚ÄìLanguage Models</h2>
    <p>
      Dezhi Luo<sup>1</sup>, Maijunxian Wang<sup>2</sup>, Bingyang Wang<sup>3*</sup>,<br>
      Tianwei Zhao<sup>4*</sup>, Yijiang Li<sup>5</sup>, Hokin Deng<sup>6</sup>
    </p>
    <p>
      <sup>1</sup>UMich/UCL, <sup>2</sup>UC Davis, <sup>3</sup>Emory,<br>
      <sup>4</sup>Johns Hopkins, <sup>5</sup>UCSD, <sup>6</sup>CMU
    </p>
    <p>
      <a href="psychophysics_paper.pdf">üìÑ Paper</a> |
      <a href="https://github.com/YOUR_REPO">üíª Code</a> |
      <a href="#bibtex">üìö BibTeX</a>
    </p>
  </header>

  <!-- Abstract -->
  <section class="section">
    <h2>Abstract</h2>
    <p>
      Cognitive control refers to the ability to flexibly coordinate thought and action in pursuit of internal goals.
      A standard method for assessing cognitive control involves conflict tasks that contrast congruent and incongruent trials,
      measuring the ability to prioritize relevant information while suppressing interference. We evaluate 108 vision-language models
      on three classic conflict tasks and their more demanding ‚Äúsquared‚Äù variants across 2,220 trials.
      Model performance corresponds closely to human behavior under resource constraints and reveals individual differences.
      These results indicate that some form of human-like executive function has emerged in current multi-modal foundation models.
    </p>
  </section>

  <!-- Figures -->
  <section class="section">
    <h2>Standard vs. Squared Tasks</h2>
    <img src="figure1.png" alt="Standard Task" />
    <p>Figure: Stroop and Flanker task examples (congruent vs. incongruent).</p>
    <img src="figure2.png" alt="Squared Task" />
    <p>Figure: Squared conflict variants introducing layered interference.</p>
    <img src="standard_vs_squared.png" alt="Condition Performance Comparison" />
    <p>Figure: Comparison across Fully Congruent, Incongruent, SCRI, SIRC conditions.</p>
  </section>

  <section class="section">
    <h2>Control Task Design</h2>
    <img src="control_tasks.png" alt="Control Task Diagram" />
    <p>
      Control tasks isolate OCR, color recognition, and spatial attention. Results show models perform near-ceiling under control,
      confirming interference effects stem from representational conflict.
    </p>
  </section>

  <section class="section">
    <h2>Scaling Analysis</h2>
    <img src="scaling.png" alt="Scaling Effect Plot" />
    <p>
      As model size increases, accuracy on incongruent and squared trials improves.
      Larger models exhibit reduced conflict sensitivity, reflecting emergence of cognitive control mechanisms.
    </p>
  </section>

  <!-- BibTeX -->
  <section class="section" id="bibtex">
    <h2>BibTeX</h2>
    <pre><code>@inproceedings{wang2025psychophysics,
  title={Machine Psychophysics: Cognitive Control in Vision-Language Models},
  author={Luo, Dezhi and Wang, Maijunxian and Wang, Bingyang and Zhao, Tianwei and Li, Yijiang and Deng, Hokin},
  booktitle={NeurIPS},
  year={2025}
}</code></pre>
  </section>

  <!-- Footer -->
  <footer>
    <p>¬© 2025 Machine Psychophysics Project | Built with ‚ù§Ô∏è | Template adapted from MQT-LLaVA</p>
  </footer>

</body>
</html>
