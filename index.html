<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="description" content="Machine Psychophysics: Cognitive Control in Vision–Language Models" />
  <meta name="keywords" content="Cognitive Control, Vision-Language Models, VLM, Psychophysics, AI, Executive Function" />
  <meta property="og:image" content="./static/images/teaser.png" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Machine Psychophysics: Cognitive Control in Vision–Language Models</title>

  <!-- Twitter card -->
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="Machine Psychophysics: Cognitive Control in VLMs" />
  <meta name="twitter:description" content="We evaluate 108 vision-language models across cognitive control tasks and reveal emergent human-like executive function." />
  <meta name="twitter:image" content="./static/images/teaser.png" />

  <!-- Styles -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />
  <link rel="stylesheet" href="./static/css/bulma.min.css" />
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css" />
  <link rel="stylesheet" href="./static/css/index.css" />
  <link rel="icon" href="./static/images/favicon.svg" />
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container has-text-centered">
        <h1 class="title is-1">Machine Psychophysics</h1>
        <h2 class="subtitle is-3">Cognitive Control in Vision–Language Models</h2>
        <p class="is-size-5">
          Dezhi Luo<sup>1</sup>, Maijunxian Wang<sup>2</sup>, Bingyang Wang<sup>3*</sup>,<br>
          Tianwei Zhao<sup>4*</sup>, Yijiang Li<sup>5</sup>, Hokin Deng<sup>6</sup>
        </p>
        <p class="is-size-6">
          <sup>1</sup>UMich / UCL, <sup>2</sup>UC Davis, <sup>3</sup>Emory,<br>
          <sup>4</sup>Johns Hopkins, <sup>5</sup>UCSD, <sup>6</sup>CMU
        </p>
        <br>
        <div class="buttons is-centered">
          <a href="./static/pdf/psychophysics_paper.pdf" class="button is-dark is-rounded">
            <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Paper</span>
          </a>
          <a href="https://github.com/YOUR_REPO" class="button is-dark is-rounded">
            <span class="icon"><i class="fab fa-github"></i></span><span>Code</span>
          </a>
          <a href="#bibtex" class="button is-dark is-rounded">
            <span class="icon"><i class="fas fa-quote-right"></i></span><span>BibTeX</span>
          </a>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container">
      <h2 class="title is-3">Abstract</h2>
      <p class="content">
        Cognitive control refers to the ability to flexibly coordinate thought and action in pursuit of internal goals.
        We evaluate 108 vision-language models on three classic conflict tasks and their more demanding “squared” variants across 2,220 trials.
        Model performance corresponds closely to human behavior under resource constraints and reveals individual differences.
        These results indicate that some form of human-like executive function has emerged in current multi-modal foundation models.
      </p>
    </div>
  </section>

  <section class="section">
    <div class="container">
      <h2 class="title is-3">Standard vs. Squared Tasks</h2>
      <div class="columns">
        <div class="column">
          <figure class="image"><img src="./static/images/figure1.png" alt="Standard Tasks"></figure>
          <p class="has-text-centered">Figure: Stroop and Flanker tasks</p>
        </div>
        <div class="column">
          <figure class="image"><img src="./static/images/figure2.png" alt="Squared Tasks"></figure>
          <p class="has-text-centered">Figure: Squared variants with layered conflict</p>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container">
      <h2 class="title is-3">Control Task Design</h2>
      <figure class="image"><img src="./static/images/control_tasks.png" alt="Control Tasks"></figure>
      <p class="content has-text-justified">
        Control tasks isolate OCR, color perception, and spatial recognition to rule out low-level deficiencies.
        All models performed at or near ceiling in these tasks, confirming that errors in the main tasks arise from representational conflict.
      </p>
    </div>
  </section>

  <section class="section">
    <div class="container">
      <h2 class="title is-3">Scaling Analysis</h2>
      <figure class="image"><img src="./static/images/scaling.png" alt="Scaling Results"></figure>
      <p class="content has-text-justified">
        Model performance across different scales reveals human-like congruency effects on classic and squared tasks.
        Small models succeed in congruent cases but fail under conflict, whereas large models resolve interference more effectively.
      </p>
    </div>
  </section>

  <section class="section">
    <div class="container">
      <h2 class="title is-3">Key Insight</h2>
      <p class="content has-text-justified">
        Our study suggests that cognitive control mechanisms—previously thought to require symbolic modeling—can emerge from scaled associative systems trained via general-purpose learning.
        This bridges the gap between symbolic cognitive architectures and neural network-based models of intelligence.
      </p>
    </div>
  </section>

  <section class="section">
    <div class="container">
      <h2 class="title is-3">Acknowledgments</h2>
      <p class="content">
        We thank Han Zhang, Jacob Sellers, and Sarah Liberatore for insightful comments. Thanks to <a href="https://zillion.network">Zillion Network Inc.</a> for providing compute infrastructure.
      </p>
    </div>
  </section>

  <section class="section" id="bibtex">
    <div class="container content">
      <h2 class="title has-text-centered">BibTeX</h2>
      <pre><code>@inproceedings{wang2025psychophysics,
  title     = {Machine Psychophysics: Cognitive Control in Vision-Language Models},
  author    = {Luo, Dezhi and Wang, Maijunxian and Wang, Bingyang and Zhao, Tianwei and Li, Yijiang and Deng, Hokin},
  booktitle = {NeurIPS},
  year      = {2025}
}</code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="content has-text-centered">
      <p>Built with ❤️ | Based on SpatialVLM template</p>
    </div>
  </footer>
</body>
</html>
