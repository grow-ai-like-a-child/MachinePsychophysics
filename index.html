<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">

  <!-- â€”â€” Meta â€”â€” -->
  <meta name="description" content="Large-scale psychophysics study shows emergent cognitive-control signatures in 108 Visionâ€“Language Models across 2 220 trials (+ 238 control).">
  <meta property="og:title" content="Machine Psychophysics: Cognitive Control in Visionâ€“Language Models">
  <meta property="og:description" content="108 VLMs Ã— 2 220 trials (plus 238 control) reveal human-aligned congruency effects and persistent hierarchical-conflict deficits.">
  <meta property="og:url" content="https://github.com/grow-ai-like-a-child/Psychophysics.git">
  <meta property="og:image" content="standard_vs_squared.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta name="twitter:title" content="Machine Psychophysics: Cognitive Control in VLMs">
  <meta name="twitter:description" content="Emergent executive-function signatures and scaling trends across 108 VLMs.">
  <meta name="twitter:image" content="standard_vs_squared.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Machine Psychophysics: Cognitive Control in Visionâ€“Language Models</title>

  <!-- Fonts & Bulma -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.3/dist/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

  <!-- libs -->
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/js/all.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.3/dist/js/bulma-carousel.min.js"></script>

<!-- â€”â€” æ ·å¼ â€”â€” -->
<style>
  :root{
    --bg-main:#ffffff;
    --text-main:#1a1a1a;
  }
  [data-theme="dark"]{
    --bg-main:#181818;
    --text-main:#e5e5e5;
  }
  html{scroll-behavior:smooth}
  body{background:var(--bg-main);color:var(--text-main);font-family:"Noto Sans",sans-serif}

  /* åŸæœ‰æ ·å¼ */
  .navbar{position:sticky;top:0;z-index:20;background:var(--bg-main);box-shadow:0 2px 4px rgba(0,0,0,.06)}
  .hero-body{padding:3rem 1.5rem}
  .title.is-1{font-size:2.5rem!important} @media(min-width:1024px){.title.is-1{font-size:3rem!important}}
  @media(min-width:1440px){.title.is-1{font-size:3.5rem!important}}

  /* æ·±è‰²æ ‡é¢˜ç™½å­— */
  [data-theme="dark"] .title{color:#ffffff!important;}

  /* æ·±è‰²ä¸‹ .hero.is-light ä¿®æ­£ */
  [data-theme="dark"] .hero.is-light{background:#1e1e1e!important;color:#e5e5e5!important;}

  /* â˜… NEW æ·±è‰²ä¸‹ æ‰€æœ‰ strong/figcaption çš„æ–‡å­—é¢œè‰² */
  [data-theme="dark"] p strong,
  [data-theme="dark"] li strong,
  [data-theme="dark"] figcaption strong{color:#ffffff!important;}
  [data-theme="dark"] figcaption{color:#dcdcdc!important;}

  /* Key Findings æ¸å˜å¡ç‰‡ */
  .gradient-box{position:relative;color:#f5f5f5!important;text-shadow:0 1px 3px rgba(0,0,0,.4)}
  .gradient-box::before{content:"";position:absolute;inset:0;border-radius:inherit;
    background:rgba(0,0,0,.15);mix-blend-mode:multiply;pointer-events:none}

  /* Design æ®µè½ <u> å»ä¸‹åˆ’çº¿ */
  #design u{text-decoration:none;}

  /* å›¾åƒç¾åŒ– */
  .carousel img,figure img{border-radius:8px;box-shadow:0 4px 8px rgba(0,0,0,.08)}
  .box{border-radius:6px;box-shadow:0 2px 6px rgba(0,0,0,.05)}
  figure figcaption{margin-top:6px;font-size:.8rem;color:#666;text-align:center}

  /* BibTeX copy */
  .copy-btn{position:absolute;top:8px;right:8px;border:none;background:#3273dc;color:#fff;
    border-radius:4px;font-size:.8rem;padding:4px 8px;cursor:pointer}

  /* â€”â€” æ–°å¢ä»£ç ï¼šæ·±è‰²æ¨¡å¼ä¸‹é¡µè„šèƒŒæ™¯å’Œé“¾æ¥é¢œè‰²ä¿®æ­£ â€”â€” */
  [data-theme="dark"] .footer {
    background-color: #181818 !important;
    color: #e5e5e5 !important;
  }

  [data-theme="dark"] .footer a {
    color: #a0c8ff !important;
  }
</style>
</head>
<body>

<!-- Sticky Nav -->
<nav class="navbar" role="navigation">
  <div class="container is-max-desktop" style="justify-content:space-between">
    <div class="navbar-brand">
      <a class="navbar-item" href="#">VLM-Psychophysics</a>
      <a role="button" class="navbar-burger" data-target="navMenu">
        <span></span><span></span><span></span>
      </a>
    </div>
    <div id="navMenu" class="navbar-menu">
      <a class="navbar-item" href="#abstract">Abstract</a>
      <a class="navbar-item" href="#design">Design</a>
      <a class="navbar-item" href="#results">Results</a>
      <a class="navbar-item" href="#BibTeX">BibTeX</a>
    </div>
    <button id="theme-toggle" class="button is-small is-rounded">
      <span class="icon"><i class="fas fa-moon"></i></span>
    </button>
  </div>
</nav>

<!-- Hero -->
<section id="top" class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
      <h1 class="title is-1">ğŸ§  Machine Psychophysics: Cognitive Control in Visionâ€“Language Models</h1>

      <p class="is-size-5">Dezhi Luo<sup>1</sup>, Maijunxian Wang<sup>2</sup>, Bingyang Wang<sup>3</sup>,<br>
        Tianwei Zhao<sup>4</sup>, Yijiang Li<sup>5</sup>, Hokin Deng<sup>6</sup></p>
      <p class="is-size-6">
        <sup>1</sup>Michigan <sup>2</sup>UC Davis <sup>3</sup>Emory <sup>4</sup>JHU <sup>5</sup>UCSD <sup>6</sup>CMU
      </p>
      <p class="is-size-6">ğŸŒ± <strong>GrowAI Team</strong> â€“
        <a href="https://growing-ai-like-a-child.github.io/" target="_blank">growing-ai-like-a-child.github.io</a>
      </p>

      <p style="margin-top:15px">
        <a class="button is-dark is-rounded" href="https://arxiv.org/abs/2505.18969" target="_blank">
          <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Paper</span></a>
        <a class="button is-dark is-rounded" href="https://github.com/grow-ai-like-a-child/Psychophysics" target="_blank">
          <span class="icon"><i class="fab fa-github"></i></span><span>Code & Dataset</span></a>
      </p>
    </div>
  </div>
</section>

<!-- Teaser -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="notification is-primary is-light" style="font-size:1.1rem">
      <strong>TL;DR</strong> 108 VLMs reproduce classic congruency effects on Stroop & Flanker tasks, yet collapse on hierarchical â€œSquaredâ€ conflicts â€” robust executive control still lags behind even in the largest models.
    </div>
  </div>
</section>

<!-- Abstract -->
<section id="abstract" class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p><strong>Cognitive control</strong> refers to the ability to flexibly coordinate thought and action. Conflict-task paradigms benchmark this faculty by contrasting congruent and incongruent trials.</p>
          <p>We conduct the first <strong>large-scale psychophysics evaluation</strong> of Visionâ€“Language Models, testing <strong>108 models</strong> on Stroop, Letter- and Number-Flanker tasks plus their more demanding â€œ<em>Squared</em>â€ variants â€” <strong>across 2 220 trials, plus 238 control trials</strong>.</p>
          <p>Models show human-like congruency patterns, yet collapse when conflicts are hierarchical. Accuracy grows <em>roughly log-linearly</em> with parameter count, echoing resource-limited curves in human forced-response studies.</p>
          <p>The benchmark demonstrates that executive-function signatures can emerge from general-purpose learning at scale while revealing clear gaps for future research.</p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Key Findings -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered" style="margin-top:2rem;">ğŸ” Key Findings</h2>

      <div class="columns is-multiline">
        <div class="column is-half">
          <div class="box gradient-box" style="background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);">
            <h3 class="title is-4"><i class="fas fa-sliders-h"></i> Emergent Congruency</h3>
            <p>All models reproduce the classic congruency effect, indicating interference resolution emerges from multimodal pre-training.</p>
          </div>
        </div>
        <div class="column is-half">
          <div class="box gradient-box" style="background:linear-gradient(135deg,#ffecd2 0%,#fcb69f 100%);">
            <h3 class="title is-4"><i class="fas fa-layer-group"></i> Hierarchical Interference</h3>
            <p>Squared tasks introduce nested conflicts still unsolved by top-tier systems.</p>
          </div>
        </div>
        <div class="column is-half">
          <div class="box gradient-box" style="background:linear-gradient(135deg,#a8edea 0%,#fed6e3 100%);">
            <h3 class="title is-4"><i class="fas fa-chart-line"></i> Scaling Trend</h3>
            <p>Performance rises <strong>roughly log-linearly</strong> from 1 B â†’ 110 B parameters, paralleling human processing-time curves.</p>
          </div>
        </div>
        <div class="column is-half">
          <div class="box gradient-box" style="background:linear-gradient(135deg,#ff9a9e 0%,#fecfef 100%);">
            <h3 class="title is-4"><i class="fas fa-check-double"></i> Convergent Validity</h3>
            <p>Letter- and Number-Flanker scores strongly covary, confirming a unified control construct.</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Experiment Overview -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">ğŸ§ª Experiment Overview</h2>

    <div class="columns is-multiline" style="margin-bottom: 2rem;">
      <div class="column is-one-third has-text-centered">
        <div class="has-text-weight-bold is-size-1" style="color: #f39c12;">~85%</div>
        <div class="has-text-weight-semibold">Control Task Accuracy</div>
        <div class="is-size-7 has-text-grey">(Perception-only tasks)</div>
      </div>
      <div class="column is-one-third has-text-centered">
        <div class="has-text-weight-bold is-size-1" style="color: #e74c3c;">108</div>
        <div class="has-text-weight-semibold">Visionâ€“Language Models</div>
        <div class="is-size-7 has-text-grey">(1Bâ€“110B parameters)</div>
      </div>
      <div class="column is-one-third has-text-centered">
        <div class="has-text-weight-bold is-size-1" style="color: #27ae60;">2,220</div>
        <div class="has-text-weight-semibold">Conflict Trials</div>
        <div class="is-size-7 has-text-grey">(Stroop, Flanker, Squared)</div>
      </div>
    </div>

    <p class="has-text-centered is-size-5">
      We designed a multi-tier psychophysics battery, spanning <strong>classic and hierarchical conflict tasks</strong>, tested across <strong>108 Visionâ€“Language Models</strong> using a total of <strong>2,220 trials</strong>.
    </p>
  </div>
</section>

  
<!-- Experimental Design -->
<section id="design" class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered" style="margin-top:2rem;">ğŸ”¬ Experimental Design</h2>
      <div class="content has-text-justified" style="max-width:800px;margin:0 auto 1.5rem">
        <p><strong>We evaluate cognitive control in VLMs with a three-tier battery.</strong>
          <br><b>(1) Standard tasks</b> reproduce classic Stroop and Flanker paradigms â€” models must ignore irrelevant colour or flanker cues in simple congruent vs. incongruent trials. <u>They include <strong>84 Stroop</strong> (42&nbsp;C/42&nbsp;I) and <strong>180 Letter- / 180 Number-Flanker</strong> (90&nbsp;C/90&nbsp;I) stimuli.</u>
          <br><b>(2) Squared tasks</b> add a second layer of conflict, yielding four hierarchical conditions (FC, FI, SCRI, SIRC) that tax executive control far beyond the standard benchmark. <u>This yields <strong>336 Stroop-Squared</strong> and <strong>720 stimuli for each Flanker-Squared task</strong>.</u>
          <br><b>(3) Control battery</b> disentangles low-level demands by isolating OCR, colour perception, and 2-D spatial encoding, ensuring that any deficits arise from representational conflict rather than perception per se. <u>It contains <strong>238 control trials</strong>.</u>
        </p>
      </div>

      <div id="methodology-carousel" class="carousel results-carousel">
        <div class="item">
          <figure>
            <img loading="lazy" src="figure1.png" alt="Standard tasks examples">
            <figcaption><strong>Standard Tasks.</strong> 84 Stroop + 180 Ã— 2 Flanker stimuli.</figcaption>
          </figure>
        </div>
        <div class="item">
          <figure>
            <img loading="lazy" src="figure2.png" alt="Squared tasks conditions">
            <figcaption><strong>Squared Tasks.</strong> 336 Stroop-Squared + 720 Ã— 2 Flanker-Squared stimuli.</figcaption>
          </figure>
        </div>
        <div class="item">
          <figure>
            <img loading="lazy" src="control_tasks.png" alt="Control battery">
            <figcaption><strong>Control Battery.</strong> 238 trials disentangling OCR, colour, spatial encoding.</figcaption>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Results -->
<section id="results" class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">ğŸ“Š Results</h2>
    <div class="content has-text-justified">
      <h3 class="title is-4">Highlights</h3>
      <ul>
        <li>Standard tasks reproduce classic congruency patterns.</li>
        <li>Squared tasks expose substantial hierarchical-interference cost.</li>
        <li>Control battery confirms deficits stem from conflict, not perception.</li>
        <li>Performance improves steadily with parameter count.</li>
      </ul>

      <figure style="text-align:center;margin:2rem 0">
        <img loading="lazy" src="standard_vs_squared.png" alt="Overall accuracy" style="max-width:95%">
        <figcaption><strong>Overall Results.</strong> Human-model accuracy across conflict conditions.</figcaption>
      </figure>

      <h4 class="title is-5">Scaling with parameters</h4>
      <p style="max-width:720px;margin:0 auto 1rem">
        Model size acts as a proxy for available computational resources. Accuracy grows <strong>roughly log-linearly</strong> from 1&nbsp;B to 110&nbsp;B parameters, yet even the largest models fall short of human performance on hierarchical conflict.
      </p>

      <figure style="text-align:center">
        <img loading="lazy" src="scaling.png" alt="Scaling curve" style="max-width:90%">
        <figcaption><strong>Scaling Trend.</strong> Accuracy vs. parameter count (1&nbsp;B â†’ 110&nbsp;B).</figcaption>
      </figure>
    </div>
  </div>
</section>

<!-- BibTeX -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content" style="position:relative">
    <h2 class="title">BibTeX</h2>
    <button class="copy-btn" id="copyBtn">Copy</button>
<pre><code id="bibtex">
@article{luo2025machine,
  title   = {Machine Psychophysics: Cognitive Control in Visionâ€“Language Models},
  author  = {Luo, Dezhi and Wang, Maijunxian and Wang, Bingyang and Zhao, Tianwei and Li, Yijiang and Deng, Hokin},
  publisher={arXiv Preprints},
  year    = {2025},
  url     = {https://arxiv.org/abs/2505.18969}
}
</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p><strong>ğŸŒ± GrowAI Team</strong> â€” <a href="https://growing-ai-like-a-child.github.io/" target="_blank">growing-ai-like-a-child.github.io</a></p>
      <p style="font-size:.9rem">Page adapted from 
        <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>. 
        Licensed under <a href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">CC BY-SA 4.0</a>.
      </p>
    </div>
  </div>
</footer>

<!-- JS -->
<script>
  /* burger */
  const burger=document.querySelector('.navbar-burger');
  burger.onclick=()=>{burger.classList.toggle('is-active');document.getElementById(burger.dataset.target).classList.toggle('is-active');};

  /* theme */
  const root=document.documentElement,btn=document.getElementById('theme-toggle');
  if(localStorage.theme==='dark'){root.dataset.theme='dark';btn.innerHTML='<span class="icon"><i class="fas fa-sun"></i></span>';}
  btn.onclick=()=>{const dark=root.dataset.theme==='dark';root.dataset.theme=dark?'light':'dark';
    btn.innerHTML=dark?'<span class="icon"><i class="fas fa-moon"></i></span>':'<span class="icon"><i class="fas fa-sun"></i></span>';
    localStorage.theme=root.dataset.theme;};

  /* copy bib */
  document.getElementById('copyBtn').onclick=()=>{
    navigator.clipboard.writeText(document.getElementById('bibtex').innerText).then(()=>{
      const b=document.getElementById('copyBtn');b.textContent='Copied!';setTimeout(()=>b.textContent='Copy',1500);
    });
  };

  /* carousel */
  document.addEventListener('DOMContentLoaded',()=>bulmaCarousel.attach());
</script>
</body>
</html>
